{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Νευρωνικά Δίκτυα με PyTorch\n",
    "\n",
    "Τα δίκτυα βαθιάς μάθησης τείνουν να είναι τεράστια με δεκάδες ή εκατοντάδες επίπεδα, για αυτό και ο όρος «βαθιά». Μπορείτε να δημιουργήσετε ένα τέτοιο βαθύ δίκτυο χρησιμοποιώντας μόνο πίνακες με βάρη όπως κάναμε στην προηγούμενη ενότητα, αλλά γενικά είναι πολύ πολύπλοκο και δύσκολο στην εφαρμογή του. Η PyTorch διαθέτει ένα βολικό module `nn` που παρέχει έναν ωραίο τρόπο για την αποτελεσματική υλοποίηση μεγάλων νευρωνικών δικτύων.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import των απαραίτητων βιλιοθηκών\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import helper\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα θα υλοποιήσουμε ένα μεγαλύτερο δίκτυο που μπορεί να λύσει ένα (πρώην) δύσκολο πρόβλημα, αναγνωρίζοντας χαρακτήρες κειμένου απο μια εικόνα. Στο πρόβλημα αυτό θα χρησιμοποιήσουμε τη βάση δεδομένων MNIST που αποτελείται από χειρόγραφα ψηφία σε αποχρώσεις του γκρι. Κάθε εικόνα είναι 28x28 pixel, και μπορείτε να δείτε ένα δείγμα παρακάτω\n",
    "\n",
    "<img src='assets/mnist.png'>\n",
    "\n",
    "Στόχος μας είναι να δημιουργήσουμε ένα νευρωνικό δίκτυο που μπορεί να δεκετεί σαν είσοδο μία από αυτές τις εικόνες και να προβλέψει το ψηφίο της εικόνας.\n",
    "\n",
    "Πρώτα από όλα, πρέπει να κατεβάσουμε το σύνολο των δεδομένων μας. Αυτό γίνεται διαθέσιμο μέσω του package `torchvision`. Ο παρακάτω κώδικας θα κατεβάσει όλη τη βάση δεδομένων MNIST, και στη συνέχεια θα δημιουργήσει σύνολα δεδομένων εκπαίδευσης και ελέγχου για εμάς. Μην ανησυχείτε πάρα πολύ για τις λεπτομέρειες εδώ, θα μάθετε περισσότερα για αυτό αργότερα.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Εκτελέστε τον κώδικα σε αυτό το κελί\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Καθορίζω ένα μετασχηματισμό (transform) για κανονικοποίηση των δεδομένων\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Κατε΄βαζω και φορτώνω τα δεδομένα εκπαίδευσης\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Έχουμε τα δεδομένα εκπαίδευσης φορτωμένα στον `trainloader` και δημιουργούμε έναν iterator με το `iter(trainloader)`. Αργότερα, θα το χρησιμοποιήσουμε σε ένα επαναληπτικό βρόχο για το σύνολο των δεδομένων εκπαίδευσης, όπως παρακάτω\n",
    "\n",
    "```python\n",
    "for image, label in trainloader:\n",
    "    ## θα εκτελέσουμε διάφορες εντολές πανω στις εικόνας (images) και τις ετικέτες (labels)\n",
    "```\n",
    "\n",
    "Θα παρατηρήσετε ότι δημιούργησα το `trainloader` με μέγεθος batch 64 και `shuffle=True`. Το μέγεθος του batch είναι ο αριθμός των εικόνων που χρησιμοποιούμε σε μια επαναληπτική διαδικασία από τον φορτωτή δεδομένων και τα εισάγουμε στο δίκτυό μας, το υποσύνολο αυτών των εικόνων ονομάζεται *batch*. Το `shuffle=true` ανακατεύει το σύνολο δεδομένων κάθε φορά που ξεκινάμε ξανά το φορτωτή δεδομένων. Στη περίπτωση εδω θα φορτώσω ένα batch, για να ελέγξουμε και να δουμε τα δεδομένα. Μπορούμε να δούμε παρακάτω ότι το `images` είναι απλώς ένας τανυστής με μέγεθος `(64, 1, 28, 28)`. Που σημαίνει, 64 εικόνες ανά batch, 1 κανάλι χρώματος και 28x28 ανάλυση εικόνας.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Έτσι μοιάζει μία από τις εικόνες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αρχικά, ας προσπαθήσουμε να δημιουργήσουμε ένα απλό δίκτυο για αυτό το σύνολο δεδομένων χρησιμοποιώντας πίνακες βάρους και πολλαπλασιασμούς πινάκων. Στη συνέχεια, θα δούμε πώς να το κάνουμε χρησιμοποιώντας το module `nn` της PyTorch η οποία παρέχει μια πολύ πιο βολική και ισχυρή μέθοδο για τον ορισμό των αρχιτεκτονικών δικτύου.\n",
    "\n",
    "Τα δίκτυα που εχουμε δει μέχρι τώρα ονομάζονται *πλήρως συνδεδεμένα* (fully-connected) ή *πυκνά* (dense) δίκτυα. Κάθε κόμβος σε ένα επίπεδο συνδέεται με κάθε κόμβο στο επόμενο επίπεδο. Στα πλήρως συνδεδεμένα δίκτυα, η είσοδος σε κάθε επίπεδο πρέπει να ένα μονοδιάστατο διάνυσμα (το οποίο μπορεί να στοιβαχθεί σε ένα δισδιάστατο τανυστή ως μία παρτίδα (batch) πολλαπλών δειγμάτων). Ωστόσο, οι εικόνες μας είναι 28x28 2D tensors, οπότε πρέπει να τις μετατρέψουμε σε 1D διανύσματα. Μιλώντας για μεγέθη, πρέπει να μετατρέψουμε το σύνολο των εικόνων που έχουν μέγεθος \"(64, 1, 28, 28)\" σε μεγεθος \"(64, 784)\", το 784 είναι 28 φορές το 28. Αυτό ονομάζεται συνήθως *ισοπέδωση* (flattening), μετασχηματίζωντας τις 2D εικόνες σε μία επίπεδη αναπαράσταση ενος μονοδιάστατου διανύσματος.\n",
    "\n",
    "Στα προηγούμενα παραδείγματα δημιουργήσατε ένα δίκτυο με έναν κόμβο εξόδου. Εδώ χρειαζόμαστε 10 κόμβους εξόδου, έναν για κάθε ψηφίο. Θέλουμε το δίκτυό μας να προβλέψει το ψηφίο που εμφανίζεται σε μια εικόνα, οπότε αυτό που θα κάνουμε είναι να υπολογίσουμε τις πιθανότητες ότι η εικόνα απεικονίζει ένα απο τα ψηφία. Αυτό καταλήγει σε μία διακριτή κατανομή πιθανότητας για τις κλάσεις (ψηφία) δίνοντας την πιο πιθανή κλάση για την εικόνα. Για το λόγο αυτό χρειαζόμαστε 10 κόμβους εξόδου για τις 10 κλάσεις (ψηφία). Στη συνέχεια, θα δούμε πώς μετατρέπουμε την έξοδο του δικτύου σε μία κατανομή πιθανότητας.\n",
    "\n",
    "\n",
    "> **Άσκηση:** Μετασχηματίστε σε επίπεδη αναπαράσταση τη παρτίδα των εικόνων `images`. Στη συνέχεια, δημιουργήστε ένα δίκτυο πολλαπλών επιπέδων με 784 κόμβους εισόδου, 256 κρυφούς κόμβους και 10 κόμβους εξόδου χρησιμοποιώντας τυχαίους τανυστές για τα βάρη και τις πολώσεις. Προς το παρόν, χρησιμοποιήστε τη sigmoid για συνάρτηση ενεργοποίησης στο κρυφό επίπεδο. Αφήστε το επίπεδο εξόδου χωρίς συνάρτηση ενεργοποίησης, θα προσθέσουμε στη συνέχεια μία συνάρηση που θα μας δίνει μια κατανομή πιθανότητας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Λύση\n",
    "def activation(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "# Επίπεδη αναπαράσταση των εικόνων εισόδου\n",
    "inputs = images.view(images.shape[0], -1)\n",
    "\n",
    "# Δημιουργία παραμέτρων\n",
    "w1 = torch.randn(784, 256)\n",
    "b1 = torch.randn(256)\n",
    "\n",
    "w2 = torch.randn(256, 10)\n",
    "b2 = torch.randn(10)\n",
    "\n",
    "h = activation(torch.mm(inputs, w1) + b1)\n",
    "\n",
    "out = torch.mm(h, w2) + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα έχουμε 10 εξόδους για το δίκτυό μας. Θέλουμε να δώσουμε μια εικόνα στο δίκτυό μας και να υπολογίσουμε μια κατανομή πιθανότητας για τις κλάσεις οι οποίες θα ορίζουν τις ποιο πιθανές κλασεις στις οποίες ανήκει η εικόνα. Κάτι που να μοιάζει με αυτό:\n",
    "<img src='assets/image_distribution.png' width=500px>\n",
    "\n",
    "Εδώ βλέπουμε ότι η πιθανότητα για κάθε τάξη είναι περίπου η ίδια. Αυτό αντιπροσωπεύει ένα μη εκπαιδευμένο δίκτυο, δεν έχει δει ακόμα δεδομένα (δεν έχει εκπαιδευτεί), οπότε απλώς επιστρέφει μια ομοιόμορφη κατανομή με ίσες πιθανότητες για κάθε κλάση.\n",
    "\n",
    "Για τον υπολογισμό αυτής της κατανομής πιθανότητας, χρησιμοποιούμε συχνά τη [**softmax** function](https://en.wikipedia.org/wiki/Softmax_function). Η εξίσωσή της είναι αυτή:\n",
    "\n",
    "$$\n",
    "\\Large \\sigma(x_i) = \\cfrac{e^{x_i}}{\\sum_k^K{e^{x_k}}}\n",
    "$$\n",
    "\n",
    "Αυτό που κάνει είναι να περιορίζει κάθε είσοδο $x_i$ μεταξύ 0 και 1 και να κανονικοποιεί τις τιμές για να πάρουμε μια σωστή κατανομή πιθανότητας όπου όλες οι πιθανότητες αθροίζονται στη τιμή 1.\n",
    "\n",
    "> **Άσκηση:** Υλοποιήστε τη συνάρτηση `softmax` που υπολογίζει και επιστρέφει κατανομές πιθανότητας για κάθε δείγμα μίας παρτίδας δεδομένων εκπαίδευσης. Σημειώστε ότι θα πρέπει να δώσετε προσοχή στα μεγέθη των τανυστών όταν το κάνετε αυτό. Εάν έχετε έναν τανυστή `a` με μέγεθος `(64, 10)` και έναν τανυστή `b` με μέγεθος `(64,)`, η πράξη `a/b` θα σας επιστρέψει σφάλμα επειδή η PyTorch θα προσπαθήσει να κάνει τη διαίρεση στις στήλες (broadcasting) και θα επιστρέψει μια αναντιστοιχία μεγέθους. Ο τρόπος σκέψης για αυτό είναι για κάθε ένα από τα 64 παραδείγματα, θέλετε μόνο να διαιρέσετε με μια τιμή, το άθροισμα του παρονομαστή. Επομένως, το `b` πρέπει να έχει μέγεθος `(64, 1)`. Με τον τρόπο αυτό, η PyTorch θα διαιρέσει τις 10 τιμές σε κάθε γραμμή του `a` με τη μία τιμή της κάθε γραμμής του `b`. Δώστε επίσης προσοχή στο πώς υπολογίζετε το άθροισμα. Θα πρέπει να ορίσετε την `dim` παράμετρο στο `torch.sum`. Η τιμή `dim=0` υπολογίζει το άθροισμα στις γραμμές ενώ το`dim=1` υπολογίζει το άθροισμα στις στήλες.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Λύση\n",
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)\n",
    "\n",
    "probabilities = softmax(out)\n",
    "\n",
    "# Έχει το σωστό μέθεθος? Πρέπει να είναι (64, 10)\n",
    "print(probabilities.shape)\n",
    "# Αθροίζονται στη τιμή ένα?\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Υλοποιώντας δίκτυα με PyTorch\n",
    "\n",
    "Η PyTorch παρέχει ένα module το `nn` που κάνει την υλοποίηση πολύ απλούστερη. Εδώ θα σας δείξω πώς να υλοποίησετε το ίδιο δίκτυο όπως το προηγούμενο με 784 εισόδους, 256 κρυφούς κόμβους, και 10 κόμβους εξόδου και έξοδο softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Η είσοδοι στο κρυφό επ΄ίπεδο γραμμικού μετασχηματισμού\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        # Επίπεδο εξόδου, 10 κόμβοι - ένας για κάθε ψηφίο\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "        # Οριζω τη συναρτηση ενεργοποίησης sigmoid και την συνάρτηση ενεργοποίησης εξόδου softmax \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Εισάγω τον τανυστή εισόδου σε κάθε μία απο τις απαραίτητες πράξεις\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ας δούμε τί γίνεται βήμα βήμα.\n",
    "\n",
    "```python\n",
    "class Network(nn.Module):\n",
    "```\n",
    "\n",
    "Δημιουργία\tκλάσης\tμε κληρονομικότητα απο το `nn.Module`. Σε συνδιασμό με το `super().__init__()` δημιουργεί μια κλάση που παρακολουθεί την αρχιτεκτονική και παρέχει πολλές χρήσιμες μεθόδους και χαρακτηριστικά. Είναι υποχρεωτικό να κληρονομήσω απο το `nn.Module` όταν φτιάχνω μία κλάση για το δίκτυό μου. Το όνομα της ίδιας της κλάσης μπορεί να είναι οτιδήποτε.\n",
    "\n",
    "```python\n",
    "self.hidden = nn.Linear(784, 256)\n",
    "```\n",
    "\n",
    "Αυτή η γραμμή δημιουργεί ένα module για το γραμμικό μετασχηματισμό, $x\\mathbf{W} + b$, με 784 εισόδους και 256 εξόδους και τα εκχωρεί στο `self.hidden`. Το module αυτόματα δημιουργεί τους πίνακες (τανυστές) weight και bias τους οποίους θα χρησιμοποιήσουμε στο μέθοδο `forward`. Μπορείτε να αποκτήσετε πρόσβαση στους τανυστές αυτούς μόλις το δίκτυο (`net`) δημιουργηθεί με τη βοήθεια των `net.hidden.weight` και `net.hidden.bias`.\n",
    "\n",
    "```python\n",
    "self.output = nn.Linear(256, 10)\n",
    "```\n",
    "\n",
    "Ομοίως, αυτή τη γραμμή δημιουργεί έναν άλλο γραμμικό μετασχηματισμό με 256 εισόδους και 10 εξόδους.\n",
    "\n",
    "```python\n",
    "self.sigmoid = nn.Sigmoid()\n",
    "self.softmax = nn.Softmax(dim=1)\n",
    "```\n",
    "\n",
    "Εδώ ορίζω την λειτουργία (operation) για τη συναρτηση ενεργοποίησης sigmoid και την softmax για την έξοδο. Ορίζοντας `dim=1` στο `nn.Softmax(dim=1)` υπολογίζω το softmax στις στήλες.\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "```\n",
    "\n",
    "Τα δίκτυα στη PyTorch που δημιουργούνται απο τη κλάση `nn.Module` πρέπει να εχουν μία μέθοδο `forward` καθορισμένη. Δέχονται σαν είσοδο εναν τανυστή `x` και τον προοθούν μέσα απο τις λειτουργίες που έχουμε ορίσει στη μέθοδο `__init__`.\n",
    "\n",
    "```python\n",
    "x = self.hidden(x)\n",
    "x = self.sigmoid(x)\n",
    "x = self.output(x)\n",
    "x = self.softmax(x)\n",
    "```\n",
    "\n",
    "Εδώ ο τανυστής εισόδου `x` περνάει μέσα απο κάθε μία λειτουργία και επανεκχωρείτε ως `x`. Βλέπουμε οτι ο τανυστής εισόδου περναέι μέσα απο το κρυφό επίπεδο, μετά απο τη σιγμοειδή συνάρτηση, μετά απο το επίπεδο εξόδου και τέλος απο τη συναρτηση  softmax. Δεν έχει σημασία πως θα ονομάσουμε τις μεταβλητές, παρα μόνο να ταιριάζουν οι είσοδοι και οι έξοδοι με τν αρχιτεκτονική του δικτύου μας. Δεν έχει σημασία και η σειρά με την οποία ορίζουμε τα επίπεδα στη μέθοδο `__init__`, αλλά πρέπει να περνάμε σωστά τα δεδομένα μας απο κάθε επίπεδο με τη σωστή σειρά στη μέθοδο `forward`.\n",
    "\n",
    "Τωρα μπορούμε να δημιουργήσουμε το αντικείμενο (object) `Network`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Δημιουργώ το δίκτυο και βλέπω την αρχιτεκτονική του\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μπορείτε να ορίσετε το δίκτυο κάπως πιο συνοπτικά και απλά χρησιμοποιώντας το module `torch.nn.functional`. Αυτός είναι ένας πιο συνηθισμένος τρόπος που ορίζονται τα δίκτυα, καθώς πολλές πράξεις τους είναι απλές συναρτήσεις μεταβλητών. Συνήθως εισάγουμε αυτό το module ως `F`, `import torch.nn.functional as F`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Είσοδοι για το κρυφό επίπεδο γραμμικού μετασχηματισμού\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        # Επίπεδο εξόδου, 10 κόμβοι - ένας για κάθε ψηφίο\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Κρυφά επιπεδα με σιγμοειδή συνάρτηση ενεργοποίησης\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        # Επίπεδο εξόδου με συνάρτηση ενεργοποίησης softmax \n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Συναρτήσεις Ενεργοποίησης\n",
    "\n",
    "Μέχρι τώρα είδαμε μόνο την ενεργοποίηση softmax, αλλά γενικά οποιαδήποτε συνάρτηση μπορεί να χρησιμοποιηθεί ως συνάρτηση ενεργοποίησης. Η μόνη απαίτηση είναι ότι για ένα δίκτυο που προσεγγίζει μια μη γραμμική συνάρτηση, οι λειτουργίες ενεργοποίησης πρέπει να είναι και αυτές μη γραμμικές.\n",
    "Ακολουθούν μερικά ακόμη παραδείγματα κοινών συναρτήσεων ενεργοποίησης: Tanh (hyperbolic tangent), και ReLU (rectified linear unit).\n",
    "\n",
    "<img src=\"assets/activation.png\" width=700px>\n",
    "\n",
    "Στην πράξη, η συνάρτηση ReLU χρησιμοποιείται σχεδόν αποκλειστικά ως συνάρτηση ενεργοποίησης για τα κρυφά επίπεδα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ένα πιο πολύπλοκο δίκτυο\n",
    "\n",
    "<img src=\"assets/mlp_mnist.png\" width=600px>\n",
    "\n",
    "> **Άσκηση:** Δημιουργείστε ένα δίκτυο με 784 κόμβους εισόδου, ένα κρυφό επίπεδο με 128 κόμβους και συνάρτηση ενεργοποίησης ReLU, μετά ένα κρυφό επίπεδο με 64 κόμβους και ReLU συνάρτηση ενεργοποίησης, και τέλος ένα επίπεδο εξόδου με συνάρτηση ενεργοποίησης softmax activation όπως παραπάνω. Μπορείτε να χρησιμοποιείσετε τη ενεργοποίηση ReLU χρησιμοποιώντας τη μέθοδο `nn.ReLU` ή τη συνάρτηση `F.relu`.\n",
    "\n",
    "Είναι καλή πρακτική να ονομάζετε τα επίπεδα σας ανάλογα με τον τύπο του δικτύου τους, για παράδειγμα 'fc' αντιστοιχεί σε fully-connected επίπεδο. Οπότε μπορω να χρησιμοποιήσω, `fc1`, `fc2`, και `fc3` ώς τα ονόματα για τα επίπεδά μου."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Λύση\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Ορίζω τα επίπεδα, 128, 64, 10 κόμβους αντίστοιχα\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # Επίπεδο εξόδου, 10 κόμβοι - ένας για κάθε ψηφίο\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Πρόσω τροφοδότηση σε όλο το δίκτυα, επιστρέφει τις εξόδους των εικόνων-ψηφίων '''\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Αρχικοποίηση weights και biases\n",
    "\n",
    "Τα βάρη και η πόλωση αρχικοποιούνται αυτόματα για εσάς, αλλά μπορείτε να προσαρμόσετε τον τρόπο με τον οποίο αρχικοποιούνται. Τα βάρη και η πόλωση είναι τανυστές συνδεδεμένοι με το επίπεδο που έχετε ορίσει, μπορείτε να τα δείτε εκτελώντας `model.fc1.weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για μή αυτόματη ενεργοποίηση, πρέπει να επέμβουμε στους ίδους τους τανυστές. Στην ουσία είναι autograd *μεταβλητές*, οπότε πρέπει πρώτα να πάρουμε τις μεταβλητές αυτές με `model.fc1.weight.data`. Όταν πάρουμε τους τανυστές, μπορούμε να τους γεμίσουμε με μηδενικά (για τις πολώσεις) ή με τυχαίες κανονικές τιμές."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ορίζω ολες τις πολώσες ως μηδέν\n",
    "model.fc1.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# κανω δειγματοληψία απο μία τυχαία κανονική κατανομη με τυπική απόκλιση = 0.01\n",
    "model.fc1.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Πρόσω τροφοδότηση\n",
    "\n",
    "Τώρα που έχουμε ένα δίκτυο, ας δούμε τι συμβαίνει όταν περνάμε σε αυτό μια εικόνα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Φόρτωσε μερικά δεδομένα \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Αλλαγή μεγέθους εικόνας σε μονοδιάστατο διάνυσμα, το νέο μέγεθος είναι (batch size, color channels, image pixels) \n",
    "images.resize_(64, 1, 784)\n",
    "# ή images.resize_(images.shape[0], 1, 784) για να πάρετε αυτόματα το μέγεθος του batch\n",
    "\n",
    "# Τροφοδότηση του δικτύου\n",
    "img_idx = 0\n",
    "ps = model.forward(images[img_idx,:])\n",
    "\n",
    "img = images[img_idx]\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Όπως βλέπετε παραπάνω, το δίκτυό μας ουσιαστικά δεν έχει ιδέα για το ποιο είναι αυτό το ψηφίο. Αυτό οφείλετε στο ότι ακόμα δεν έχει εκπαιδευτεί το δίκτυο, και όλα τα βάρη είναι τυχαία!\n",
    "\n",
    "### Χρήση του `nn.Sequential`\n",
    "\n",
    "Η PyTorch μας δίνει ενα ακόμα πιο βολικό και ευκολο τρόπο να δηιουργήσουμε τα δίκτυά μας όπως παρακάτω, όπου ο τανυστής διέρχεται διαδοχικά μεσω των απαραίτιτων πράξεων, `nn.Sequential` ([documentation](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential)). Χρησμιποιώντας αυτό το τρόπο, το ίδιο δίκτυο μπορει να δημιουργηθεί οπως παρακάτω:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Οι υπερπαράμετροι του δικτύου μας\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Δημιουργώ ενα feed-forward δίκτυο\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)\n",
    "\n",
    "# Πρόσω τροφοδότηση σε όλο το δίκτυο και απεικόνηση της εξόδου\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "helper.view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Οι πράξεις είναι διαθέσιμες δίνοντας τον κατάλληλο δείκτη. Για παράδειγμα, εάν θέλετε να εκτελέσετε το πρώτο γραμμικό μετασχηματισμό και να δείτε τα βάρη, εκτελείτε το `model[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model[0])\n",
    "model[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μπορείτε επίσης να ορίσετε `OrderedDict` για να δώσετε όνομα σε κάθε ξεχωριστό επίπεδο και στις λειτουργίες του, αντί ενός αυξητικού ακέραιου δείκτη. Σημειώστε ότι τα ονόματα πρέπει να είναι μοναδικά, επομένως _κάθε πράξη πρέπει να έχει μοναδικό όνομα_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "                      ('softmax', nn.Softmax(dim=1))]))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα μπορείτε να έχετε πρόσβαση στα επίδεδα είτε με τον αριθμό τους είτει με το όνομά τους"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model[0])\n",
    "print(model.fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στο επόμενο notebook, θα δούμε πως μπορούμε να εκπαιδεύσουμε αυτό το νευρωνικό δίκτυο για να βρίσκει με ακρίβεια σε ποιο ψηφίo αντιστοιχεί κάθε εικόνα του MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτό το notebook 📖 δημιουργήθηκε για το μάθημα ***Υπολογιστική Νοημοσύνη και Μηχανική Μάθηση*** του Τμήματος Μηχανικών Παραγωγής και Διοίκησης, της Πολυτεχνικής Σχολής του Δημοκριτείου Πανεπιστημίου Θράκης.<br>\n",
    "This notebook is made available under the Creative Commons Attribution [(CC-BY)](https://creativecommons.org/licenses/by/4.0/legalcode) license. Code is also made available under the [MIT License](https://opensource.org/licenses/MIT).<br>\n",
    "Author: Asst. Prof. Angelos Amanatiadis\n",
    "<img src=\"assets/cc.png\" style=\"width:55px; float: right; margin: 0px 0px 0px 0px;\"></img>\n",
    "<img src=\"assets/mit.png\" style=\"width:40px; float: right; margin: 0px 10px 0px 0px;\"></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
