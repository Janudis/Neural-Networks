{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Εκπαιδεύοντας τα Νευρωνικά Δίκτυα\n",
    "\n",
    "Το δίκτυο που κατασκευάσαμε στη προηγούμενη ενότητα δεν είναι ακόμα τόσο έξυπνο, καθώς δεν γνωρίζει τίποτα για τα χειρόγραφα ψηφία στις εικόνες μας. Τα νευρωνικά δίκτυα με μη γραμμικές ενεργοποιήσεις λειτουργούν σαν γενικές συναρτήσεις προσέγγισης. Υπάρχει κάποια λειτουργία που χαρτογραφεί την είσοδο με την έξοδο. Για παράδειγμα, εικόνες χειρόγραφων ψηφίων σε κλάσεις πιθανοτήτων. Η δύναμη των νευρωνικών δικτύων είναι ότι μπορούμε να τα εκπαιδεύσουμε για να προσεγγίσουμε αυτή τη λειτουργία, και γενικά κάθε συνάρτηση οταν έχουμε αρκετά δεδομένα και υπολογιστική ισχύ.\n",
    "\n",
    "<img src=\"assets/function_approx.png\" width=500px>\n",
    "\n",
    "Στην αρχή το δίκτυό μας είναι \"χαζό\", καθώς δεν γνωρίζει τη λειτουργία χαρτογράφησης/αντιστοίχησης των εισόδων με τις εξόδους. Εκπαιδεύουμε το δίκτυό μας δείχνοντάς του παραδείγματα πραγματικών δεδομένων και στη συνέχεια προσαρμόζοντας τις παραμέτρους του δικτύου έτσι ώστε να προσεγγίζει αυτή τη λειτουργία.\n",
    "\n",
    "Για να βρούμε αυτές τις παραμέτρους, πρέπει να γνωρίζουμε πόσο εσφαλμένα το δίκτυο προβλέπει τις πραγματικές εξόδους. Για αυτό υπολογίζουμε μια συνάρτηση σφάλματος **loss function** (ονομάζεται επίσης και συνάρτηση κόστους), μία μετρική δηλαδή του σφάλματος πρόβλεψης. Για παράδειγμα, το μέσο τετραγωνικό σφάλμα χρησιμοποιείται συχνά σε προβλήματα παλινδρόμησης και δυαδικής ταξινόμησης.\n",
    "\n",
    "$$\n",
    "\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "όπου $n$ είναι ο αριθμός των δειγμάτων εκπαίδευσης, $y_i$ είναι οι σωστές ετικέτες, και $\\hat{y}_i$ είναι η πρόβλεψη του δικτύου.\n",
    "\n",
    "Ελαχιστοποιώντας το σφάλμα αυτό σε συνάρτητη με τις παραμέτρους του δικτύου, μπορούμε να βρούμε παραμετροποιήσεις όπου το σφάλμα είναι στο ελάχιστο και το δίκτυο είναι σε θέση να προβλέψει τις σωστές ετικέτες με μεγάλη ακρίβεια. Μπορούμε να υπολογίσουμε αυτό το ελάχιστο χρησιμοποιώντας μια διαδικασία που ονομάζεται μεθόδος καθόδου κλίσης **gradient descent**. Η κλίση είναι η κλίση της συνάρτησης σφάλματος και δείχνει την κατεύθυνση προς τη γρηγορότερη μείωση του σφάλματος. Για να φτάσουμε στο ελάχιστο στο μικρότερο χρονικό διάστημα, τότε θέλουμε να ακολουθήσουμε την κλίση (προς τα κάτω). Μπορείτε να το σκεφτείτε σαν να κατεβαίνεται ένα βουνό ακολουθώντας την πιο απότομη πλαγιά που βλέπετε μέχρι το έδαφος.\n",
    "\n",
    "<img src='assets/gradient_descent.png' width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Οπισθοδιάδοση σφάλματος (Backpropagation)\n",
    "\n",
    "Για δίκτυα ενός επιπέδου, η κατάβαση κλίσης είναι εύκολη στην εφαρμογή. Ωστόσο, είναι πιο περίπλοκο για βαθύτερα, πολυεπίπεδα νευρωνικά δίκτυα όπως αυτό που έχουμε δημιουργήσει. Είναι τόσο περίπλοκο που χρειάστηκε σχεδόν 30 χρόνια μέχρι οι ερευνητές καταλάβουν πώς να εκπαιδεύσουν δίκτυα πολλαπλών στρωμάτων.\n",
    "\n",
    "Η εκπαίδευση δικτύων πολλαπλών επιπέδων πραγματοποιείται μέσω του **backpropagation** το οποίο είναι πραγματικά μία απλή εφαρμογή του κανόνα της αλυσίδας της άλγεβρας. Είναι πιο εύκολο να κατανοήσουμε εαν μετατρέπουμε ένα δίκτυο δύο επιπέδων σε μια απεικόνιση ενός γράφου.\n",
    "\n",
    "<img src='assets/backprop_diagram.png' width=550px>\n",
    "\n",
    "Κατα το πρώτο πέρασμα στο δίκτυο, τα δεδομένα αλλα και οι πράξεις πηγαίνουν απο κάτω προς τα επάνω στο σχήμα μας. Περνάμε την είσοδο $x$ μέσα απο ένα γραμμικό μετασχηματισμό $L_1$ με βάρη $W_1$ και πολώσεις $b_1$. Στη συνέχεια, η εξοδος αυτή περναέι μέσα απο τη σιγμοειδή συνάρτηση $S$ και απο ακόμα ένα γραμμικό μετασχηματισμό $L_2$. Τέλος υπολογίζουμε το σφάλμα $\\ell$. Χρησιμοποιούμε το σφάλμα ως μία μετρική για το πόσο καλές ή όχι είναι οι προβλέψεις του δικτύου μας. Ο στόχος μετέπειτα είναι να τροποποιήσουμε τα βάρη και τις πολώσεις έτσι ώστε να ελαχιστοποιήσουμε το σφάλμα αυτό.\n",
    "\n",
    "Για να εκπαιδεύσουμε τα βάρη με gradient descent, στην ουσία διαδίδουμε τη κλίση του σφάλματος προς τα πίσω στο δίκτυο. Κάθε λειτουργία σε ένα επίπεδο έχει μία κλίση (μερική παράγωγος) μεταξυ των εισόδων και των εξόδων. Καθως διαδίδουμε τις κλίσεις προς τα πίσω, πολλαπλάσιάζουμε την εισερχόμενη μερική παράγωγο με την μερική παράγωγο της λειτουργίας. Μαθηματικά, αυτό μπορεί να οριστεί απλά ως ο υπολογισμός της κλίσης του σφάλματος ως προς τα βάρη χρησιμοποιώντας τον Κανόνα Αλυσιδωτής Παραγώγισης.\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
    "$$\n",
    "\n",
    "**Σημείωση:** Αναφέρω εδώ μερικές λεπτομέρειες που απαιτούν γνώση άλγεβρας διανυσμάτων, αλλά δεν είναι απαραίτητες για να καταλάβετε τι ακριβώς συμβαίνει.\n",
    "\n",
    "\n",
    "Κάνουμε update τα βάρη χρησιμοποιώντας αυτήν την μερική παράγωγο μαζί με το learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "Η τιμή του learning rate $\\alpha$ επιλέγεται έτσι ώστε τα βήματα ανανέωσης των βαρών να είναι αρκετά μικρά ώστε η επαναληπτική μέθοδος να φτάνει σε ένα ελάχιστο."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Συναρτήσεις σφάλματος στη PyTorch\n",
    "\n",
    "Ας ξεκινήσουμε βλέποντας πώς υπολογίζουμε το σφάλμα με τη βοήθεια της PyTorch. Μέσω του module `nn`, η PyTorch παρέχει διάφορες συναρτήσεις σφάλματος όπως το σφάλμα διασταυρούμενης εντροπίας (` nn.CrossEntropyLoss`). Συνήθως θα δείτε τη συνάρτηση σφάλματος να αντιστοιχεί στο `criterion`. Όπως είπαμε στη προηγούμενη ενότητα, σε ένα πρόβλημα ταξινόμησης όπως το MNIST, χρησιμοποιούμε τη συνάρτηση softmax για να προβλέψουμε τις πιθανότητες κάθε κλάσης. Με μια έξοδο softmax, πρέπει να χρησιμοποιήσετε την διασταυρωμένη εντροπία ως μετρική του σφάλματος. Για να υπολογίσετε το σφάλμα, ορίστε πρώτα το `criterion` και μετά περάστε στην έξοδο του δικτύου σας και τις σωστές ετικέτες.\n",
    "\n",
    "Κάτι πολύ σημαντικό να σημειωθεί εδώ. Δειτε εδώ [documentation για τη `nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss),\n",
    "\n",
    "> Αυτό το κριτήριο συνδυάζει `nn.LogSoftmax()` και `nn.NLLLoss()` σε μία ενιαία κλάση.\n",
    ">\n",
    "> Η είσοδός του αναμένει τα σκορ της κάθε κλάσης.\n",
    "\n",
    "Αυτό σημαίνει ότι πρέπει να περάσουμε την καθαρή έξοδο του δικτύου μας στη συνάρτηση σφάλματος, και όχι την έξοδο μετά τη softmax. Αυτή η καθαρή έξοδος ονομάζεται συνήθως *logits* ή *skores*. Χρησιμοποιούμε τα logits επειδή η softmax μας δίνει πιθανότητες που συχνά είναι πολύ κοντά στο μηδέν ή το ένα, αλλά οι αριθμοί κινητής υποδιαστολής δεν μπορούν να αναπαραστήσουν με ακρίβεια τιμές κοντά στο μηδέν ή το ένα ([διαβάστε εδώ περισσότερα](https://docs.python.org/3/tutorial/floatingpoint.html)). Συνήθως είναι καλύτερο να αποφύγετε υπολογισμούς με πιθανότητες, για αυτό συνήθως χρησιμοποιούμε λογαριθμικές πιθανότητες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Καθορίστε ένα μετασχηματισμό για κανονικοποίηση των δεδομένων\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Κατεβάστε και φορτώστε τα δεδομένα εκπαίδευσης\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Δημιούργησε ενα δίκτυο εμπρóσθιας τροφοδóτησης\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "# Όρισε τη συνάρτηση σφάλματος\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Πάρτε τα δεδομένα σας\n",
    "images, labels = next(iter(trainloader))\n",
    "# Κάντε τις εικόνες μονοδιάστατο διάνυσμα \n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Πέρασμα προς τα εμπρός, πάρε την έξοδο\n",
    "logits = model(images)\n",
    "# Υπολογίστε το σφάλμα χρησιμοποιώντας την καθαρή έξοδο και τις ετικέτες\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Βάση εμπειρίας, είναι πιο βολικό να φτιάξετε ένα μοντέλο με έξοδο log-softmax `nn.LogSoftmax` ή `F.log_softmax` ([documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.LogSoftmax)). Τότε μπορείτε να πάρετε τις πραγματικές πιθανότητες παίρνοντας το εκθετικό του `torch.exp(output)`. Με έξοδο log-softmax, πρέπει να χρησιμοποιήσετε την συνάρτηση αρνητικού σφάλματος λογαριθμικής πιθανότητας, `nn.NLLLoss` ([documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss)).\n",
    "\n",
    ">**Άσκηση:** Δημιουργήστε ένα μοντέλο που επιστρέφει το log-softmax ως έξοδο και υπολογίστε την απώλεια (σφάλμα) χρησιμοποιώντας την αρνητική λογαριθμική πιθανότητα σφάλματος."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Λύση\n",
    "\n",
    "# Δημιούργησε ενα δίκτυο εμπρóσθιας τροφοδóτησης\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "# Όρισε τη συνάρτηση σφάλματος\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Πάρτε τα δεδομένα σας\n",
    "images, labels = next(iter(trainloader))\n",
    "# Κάντε τις εικόνες μονοδιάστατο διάνυσμα\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Πέρασμα προς τα εμπρός, πάρτε τις λογαριθμικ΄ές πιθανοτητες\n",
    "logps = model(images)\n",
    "# Υπολογίστε το σφάλμα χρησιμοποιώντας το logps και τις ετικέτες\n",
    "loss = criterion(logps, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Τώρα που ξέρουμε να υπολογίσουμε το σφάλμα, πως εκτελούμε το backpropagation? Η Torch διαθέτει ενα module, `autograd`, για να υπολογίζει αυτόματα τις μερικές παραγώγους των τανυστών. Μπορουμε να τη χρησιμοποιήσουμε για να υπολογίσουμε τις μερικές παραγώγους όλως των παραμέτρων σε σχέση με το σφάλμα. Η Autograd δουλεύει αποθηκεύοντας όλες τις πραξεις που έχουν γίνει στους τανυστές, στη συνέχεια κοιτάει προς τα πίσω σε αυτές τις πράξεις, και υπολογίζει όλες τις μερικές παραγώγους σε όλη τη διαδρομή απο το τέλος έως την αρχή. Για να ήμαστε σίγουροι οτι αποθηκέυει η PyTorch τις πράξεις σε ένα τανυστή και τις μερικές παραγώγους, πρέπει να ορίσετε `requires_grad = True` σε ένα τανυστή. Μπορείτε να το κάνετε αυτό κατά τη δημιουργία με το `requires_grad`, ή σε οποιαδήπτε στιγμή με το `x.requires_grad_(True)`.\n",
    "\n",
    "Μπορείτε να απενεργοποιήσετε τις παραγώγους για ένα μπλοκ κώδικα με το `torch.no_grad()` όπως παρακάτω:\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "```\n",
    "\n",
    "Επίσης, μπορείτε να ενεργοποιήσετε ή να απενεργοποιήσετε πλήρως τις παραγώγους με το `torch.set_grad_enabled(True|False)`.\n",
    "\n",
    "Οι μερικές παράγωγοι υπολογίζονται ως προς μία μεταβλητλη `z` με `z.backward()`. Αυτό εκτελεί ένα πέρασμα προς τα πίσω σε όλες τις πράξεις οι οποίες δημιούργησαν τη μεταβλητή `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρακάτω βλέπουμε τη πράξη που δημιούργησε τη `y`, μία εκθετική πράξη `PowBackward0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## grad_fn shows the function that generated this variable\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Το module autograd παρακολουθεί αυτές τις πράξεις και ξέρει πώς να υπολογίσει την παράγωγό για κάθε μία από αυτές. Με αυτό τον τρόπο, είναι σε θέση να υπολογίσει τις παραγώγους για μια αλυσίδα πράξεων, σε σχέση με ένα τανυστή. Ας μειώσουμε τη τιμή του τανυστή y στον μέσο όρο του."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μπορείτε να ελέγξετε τις παραγώγους ως προς `x` και `y` αλλά είναι κενές αυτήν τη στιγμή."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για να υπολογίσετε τις παραγώγους πρεπει να εκτελέσετε τη μέθοδο `.backward` σε μία μετάβλητη, τη `z` για παράδειγμα. Η εκτέλεση αυτή θα υπολογίσει τη μερική παράγωγο του `z` ως προς `x`.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτοί οι υπολογισμοί των μερικών παραγώγων είναι ιδιαίτερα χρήσιμοι για τα νευρωνικά δίκτυα. Για την εκπαίδευση χρειαζόμαστε τις μερικές παραγώγους των βαρών ως προς τη συνάρτηση του κόστους. Με τη PyTorch, περνάμε τα δεδομένα μας προς τα εμπρός μέσω του δικτύου και υπολογίζουμε το σφάλμα και, στη συνέχεια, πηγαίνουμε προς τα πίσω για να υπολογίσουμε τις μερικές παραγώγους σε σχέση με το σφάλμα. Μόλις έχουμε τις παραγώγους αυτές μπορούμε να κάνουμε ένα βήμα του gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Απώλεια (σφάλμα) και Autograd μαζί\n",
    "\n",
    "Όταν δημιουργούμε ένα δίκτυο με τη PyTorch, όλες οι παράμετροι αρχικοποιούνται με το `requires_grad = True`. Αυτό σημαίνει ότι όταν υπολογίζουμε το σφάλμα και εκτελούμε την συνάρτηση `loss.backward()`, υπολογίζονται οι μερικές παράγωγοι για τις παραμέτρους μας. Αυτές τιμές των παραγώγων χρησιμοποιούνται για την ενημέρωση των βαρών με gradient descent. Παρακάτω μπορείτε να δείτε ένα παράδειγμα υπολογισμού των παραγώγων χρησιμοποιώντας ένα πέρασμα προς τα πίσω."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Δημιούργησε ενα δίκτυο εμπρóσθιας τροφοδóτησης\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images)\n",
    "loss = criterion(logps, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Πρίν το πέρασμα προς τα πίσω : \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('Μετά το πέρασμα προς τα πίσω: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Εκπαιδεύοντας το δίκτυο!\n",
    "\n",
    "Υπάρχει ένα τελευταίο βήμα που πρέπει να κάνουμε πριν ξεκινήσουμε την εκπαίδευση, ένα εργαλείο βελτιστοποίησης που θα χρησιμοποιήσουμε για να ενημερώσουμε τα βάρη με τις μερικές παραγώγους. Το βήμα αυτό της ενημέρωσης γινεται στο πακετο της  PyTorch [`optim` package](https://pytorch.org/docs/stable/optim.html). Για παράδειγμα, μπορούμε να χρησιμοποιήσουμε τη στοχαστική gradient descent χρησιμοποιώντας την`optim.SGD`. Μπορείτε να δείτε πώς να ορίσετε ένα εργαλείο βελτιστοποίησης παρακάτω:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Οι βελτιστοποιητές απαιτούν τις παραμέτρους βελτιστοποίησης και ένα ρυθμό μάθησης\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα που ξέρουμε πώς να χρησιμοποιούμε όλα τα επιμέρους σκέλη, ήρθε η ώρα να δούμε πώς λειτουργούν όλα μαζί. Ας λάβουμε υπόψη μόνο ένα βήμα εκμάθησης προτού περάσουμε όλα τα δεδομένα απο το δικτυό μας. Η γενική διαδικασία στη PyTorch είναι:\n",
    "\n",
    "* Πραγματοποίησε ένα πέρασμα προς τα εμπρός στο δίκτυο \n",
    "* Χρησιμοποίησε την έξοδο του δικτύου για να υπολογίσεις το σφάλμα\n",
    "* Πραγματοποίησε ένα πέρασμα προς τα πίσω στο δίκτυο με `loss.backward()` για να υπολογίσεις τις μερικές παραγώγους\n",
    "* Πραγματοποίησε ένα βήμα στον optimizer για να ανανεώσουμε τα βάρη\n",
    "\n",
    "Παρακάτω θα προχωρήσω σε ένα βήμα εκπαίδευσης και θα εκτυπώσω τα βάρη και τις παραγώγους έτσι ώστε να δείτε πώς αλλάζουν. Σημειώστε ότι έχω εισάγει μία γραμμή κώδικα `optimizer.zero_grad ()`. Όταν κάνετε πολλαπλές αναδρομές με τις ίδιες παραμέτρους, οι παράγωγοι συσσωρεύονται. Αυτό σημαίνει ότι πρέπει να μηδενίζετε τις παραγώγους σε κάθε βήμα εκπαίδευσης αλλιώς θα διατηρείτε τις μερικές παραγώγους απο προηγούμενα βήματα εκπαίδευσης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Αρχικά βάρη - ', model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Μηδενίστε τις παραγώγους, ειναι απαραίτητο γιατι οι παράγωγοι συσσωρεύονται\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward πέρασμα, μετά backward περασμα, και ανανέωση των βαρών\n",
    "output = model(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Κάντε ένα βήμα ενημέρωσης και ενημέρωσε τα βάρη\n",
    "optimizer.step()\n",
    "print('Ανανεωμένα βάρη - ', model[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Η πραγματική εκπαίδευση\n",
    "\n",
    "Τώρα θα βάλουμε αυτόν τον αλγόριθμο σε ένα βρόχο ώστε να μπορέσουμε να χρησιμοποιήσουμε όλες τις εικόνες. Ενα πέρασμα σε ολόκληρο το σύνολο δεδομένων ονομάζεται *εποχή*. Έτσι λοιπόν, πρόκειται να έχουμε επαναλήψεις μέσω του `trainloader` για να πάρουμε όλες τις εικόνες σε παρτίδες εκπαίδευσης. Για κάθε παρτίδα, εκτελούμε ένα πέρασμα εκπαίδευσης όπου υπολογίζουμε την απώλεια (σφάλμα), εκτελούμε ενα πέρασμα προς τα πίσω και ενημερώνουμε τα βάρη.\n",
    "\n",
    "> **Άσκηση: ** Υλοποιείστε τη διαδικασία εκπαίδευσης στο δίκτυό μας. Εάν το υλοποιήσετε σωστά, θα πρέπει να δείτε την πτώση του σφάλματος εκπαίδευσης σε κάθε εποχή."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Πέρασμα εκπαίδευσης\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Με το δίκτυο εκπαιδευμένο, μπορούμε να ελέγξουμε τις προβλέψεις του."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Απενεργοποιείστε τον υπολογισμό των παραγώγων για να είναι πιο γρήγορη η εκτέλεση σε αυτό το κομμάτι του κώδικα\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "# Η έξοδος του δικτύου ειναι λογαριθμικές πιθανότητες, επομένως χρειάζεται να πάρω την εκθετική τιμή τους για τις τελικ΄ές τιμές\n",
    "ps = torch.exp(logps)\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα το δίκτυό μας είναι τέλειο. Μπορεί να προβλέψει με ακρίβεια τα ψηφία στις εικόνες μας. Στη συνέχεια, θα γράψετε τον κώδικα για την υλοποίηση ενός νευρωνικού δικτύου σε ένα πιο περίπλοκο σύνολο δεδομένων."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτό το notebook 📖 δημιουργήθηκε για το μάθημα ***Υπολογιστική Νοημοσύνη και Μηχανική Μάθηση*** του Τμήματος Μηχανικών Παραγωγής και Διοίκησης, της Πολυτεχνικής Σχολής του Δημοκριτείου Πανεπιστημίου Θράκης.<br>\n",
    "This notebook is made available under the Creative Commons Attribution [(CC-BY)](https://creativecommons.org/licenses/by/4.0/legalcode) license. Code is also made available under the [MIT License](https://opensource.org/licenses/MIT).<br>\n",
    "Author: Asst. Prof. Angelos Amanatiadis\n",
    "<img src=\"assets/cc.png\" style=\"width:55px; float: right; margin: 0px 0px 0px 0px;\"></img>\n",
    "<img src=\"assets/mit.png\" style=\"width:40px; float: right; margin: 0px 10px 0px 0px;\"></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
