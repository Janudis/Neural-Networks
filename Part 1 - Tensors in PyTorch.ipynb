{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Εισαγωγή στη Βαθιά Μάθηση με PyTorch\n",
    "\n",
    "Σε αυτό το σημειωματάριο, γίνεται η εισαγωγή στη βιβλιοθήκη [PyTorch](http://pytorch.org/), ένα σύνολο απο εργαλεία για την δημιουργία και εκπαίδευση Νευρωνικών Δικτύων. Η PyTorch κατά γενικό κανόνα παρουσιάζει πολλές ομοιότητες με τη γνωστή πλέον σε εσάς Numpy. Στην ουσία, όλοι οι πίνακες της Numpy, μπορούν να θεωρηθούν τανυστές. Η PyTorch δέχεται αυτούς τους τενσορες και με έναν εύκολο τρόπο τους διοχετεύει στη GPU για την πιο γρήγορη επεξεργασία τους, η οποία είναι απαραίτητη κατά την εκπαίδευση ενός δικτύου. Παρέχει επίσης συναρτήσεις για τον αυτόματο υπολογισμό των μερικών παραγώγων (για το βήμα του backpropagation!) και άλλες συναρτήσεις ειδικά για την δημιουργία της αρχιτεκτονικής των νευρωνικών δικτύων. Όλα μαζί αποτελούν τη PyTorch η οποία καταλήγει να είναι μία απόλυτα συμβατή βιβλιοθήκη με τη Pyhton και τη Numpy/Scipy σε σύγκριση με άλλες βιβλιοθήκες όπως η TensorFlow και άλλα frameworks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Νευρωνικά Δίκτυα\n",
    "\n",
    "Η βαθιά μάθηση βασίζεται στα τεχνητά νευρωνικά δίκτυα που πρωτοχρησιμοποιήθηκαν στην αρχική τους μορφή από τα τέλη της δεκαετίας του 1950. Τα δίκτυα αποτελούνται από μεμονωμένα δομικά στοιχεία που προσεγγίζουν τους νευρώνες, και συνήθως ονομάζονται μονάδες (units) ή \"νευρώνες\". Κάθε νευρώνας έχει έναν αριθμό σταθμισμένων εισόδων. Αυτές οι σταθμισμένες είσοδοι αθροίζονται μαζί (με βάση ένα γραμμικό συνδυασμό) και στη συνέχεια περνούν από μία συνάρτηση ενεργοποίησης για να δώσουν την έξοδο του νευρώνα.\n",
    "\n",
    "<img src=\"assets/simple_neuron.png\" width=400px>\n",
    "\n",
    "Η μαθηματική σχέση αυτή αντιστοιχεί στο παρακάτω: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Σε μορφή διανυσμάτων, αυτό αντιστοιχεί στο εσωτερικό γινόμενο (dot/inner product) των δύο διανυσμάτων:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Τανυστές\n",
    "\n",
    "Αποδεικνύεται ότι οι υπολογισμοί στα νευρωνικά δίκτυα είναι απλώς μια σειρά πράξεων γραμμικής άλγεβρας πάνω στους *τανυστές*, που αποτελούν μια γενίκευση των πινάκων. Ένα διάνυσμα είναι ένας μονοδιάστατος τανυστής, ένας πίνακας είναι ένας δισδιάστατος τανυστής, ένας πίνακας με τρεις δείκτες είναι ένας τρισδιάστατος τανυστής (για παράδειγμα, έγχρωμες εικόνες RGB). Η βασική δομή δεδομένων για τα νευρωνικά δίκτυα είναι οι τανυστές και η PyTorch (καθώς και σχεδόν κάθε άλλη βιβλιοθήκη βαθιάς μάθησης) είναι χτισμένη γύρω από τους τανυστές.\n",
    "\n",
    "<img src=\"assets/tensor_examples.svg\" width=600px>\n",
    "\n",
    "Έχοντας καλύψει τα βασικά, ήρθε η ώρα να διερευνήσουμε πώς μπορούμε να χρησιμοποιήσουμε τη PyTorch για να οικοδομήσουμε ένα απλό νευρωνικό δίκτυο."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])\n",
      "tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]])\n"
     ]
    }
   ],
   "source": [
    "### Δημιουργώ τυχαία δεδομένα\n",
    "torch.manual_seed(7) # Ορίζω random seed για να μπορω να κανω ευκολα debug\n",
    "\n",
    "# Τα δεδομένα εισόδου είναι 5 τυχαίες τιμές μίας κανονικής κατανομής\n",
    "features = torch.randn((1, 5))\n",
    "# Τα βάρη για τα δεδομένα εισόδου, πάλι απο κανονική κατανομή. Προσοχή στο _like(features)\n",
    "weights = torch.randn_like(features)\n",
    "# και τιμή για το bias\n",
    "bias = torch.randn((1, 1))\n",
    "print(features)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στις πάνω γραμμές κώδικα δημιουργήσαμε τυχαίες τιμές για να μπορέσουμε να πάρουμε τα αποτελέσματα απο ένα απλό δίκτυο. Οι τιμές είναι τυχαίες προς το παρών, αλλά όσο προχωρούμε θα αρχίζουμε να χρησιμοποιούμε κανονικά δεδομένα. Αναλύωντας τη κάθε γραμμή κώδικα:\n",
    "\n",
    "`features = torch.randn((1, 5))` δημιουργεί ένα τανυστή με μέγεθος (shape) `(1, 5)`, μία γραμμή και πέντε στήλες, ο οποίος περιέχει τυχαίες τιμές που ακολουθούν μία κανονική κατανομή μεσης τιμής μηδέν και τυπικής απόκλισης ένα. \n",
    "\n",
    "`weights = torch.randn_like(features)` δημιουργεί έναν άλλο τανυστή με μέγεθος ίδιο με τον τανυστή `features`, με τιμές ξάνα κανονικής κατανομής.\n",
    "\n",
    "Τέλος, η γραμμή κώδικα `bias = torch.randn((1, 1))` δημιουργεί μία τιμή απο μία κανονική κατανομή.\n",
    "\n",
    "Οι τανυστές στη PyTorch μπορούν να προστεθούν, να πολλαπλασιαστούν, να αφαιρεθούν κ.λπ., όπως οι αριθμητικοί πίνακες στη Numpy. Γενικά, θα χρησιμοποιείτε τους τανυστές στη PyTorch με τον ίδιο τρόπο που θα χρησιμοποιούσατε και τους αριθμητικούς πίνακες. Παρουσιάζουν όμως κάποια ωραία πλεονεκτήματα, όπως η επιτάχυνση σε GPU που θα δούμε αργότερα. Προς το παρόν, χρησιμοποιήστε τα δεδομένα που δημιουργήσατε για να υπολογίσετε την έξοδο του απλού νευρωνικού δικτύου.\n",
    "\n",
    " \n",
    "> **Άσκηση**: Υπολογίστε την έξοδο του δικτύου με είσοδο τα `features`, βάρη τα `weights`, και πόλωση/κατώφλι `bias`. Όπως στη Numpy, και η PyTorch έχει την συνάρτηση [`torch.sum()`](https://pytorch.org/docs/stable/torch.html#torch.sum), όπως και τη μέθοδο `.sum()` για τους τανυστές, για να υπολογίζονται αθροίσματα. Χρησιμοποιήστε τη συνάρτηση `activation` όπως ορίστηκε παραπάνω ως συνάρτηση ενεργοποίησης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Λ΄ύση\n",
    "\n",
    "# Υπολογίζω την έξοδο (labels) απο τα δεδομένα εισόδου και τα βάρη\n",
    "\n",
    "y = activation(torch.sum(features * weights) + bias)\n",
    "y = activation((features * weights).sum() + bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μπορείτε να κάνετε τον πολλαπλασιασμό και το άθροισμα με μία πράξη χρησιμοποιώντας πολλαπλασιασμό πινάκων. Γενικά, είναι καλό και πρέπει να χρησιμοποιείται πολλαπλασιασμούς πινάκων επειδή είναι πιο αποδοτικοί και μπορούν να επιταχυνθούν χρησιμοποιώντας σύγχρονες βιβλιοθήκες και υπολογιστές υψηλών επιδόσεων σε GPU επεξεργαστές.\n",
    "\n",
    "Εδώ, θέλουμε να κάνουμε ένα πολλαπλασιαμό πινάκων μεταξύ των features και των weights. Για την πράξη αυτή μπορούμε να χρησιμοποιήσουμε την εντολή [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) ή την [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul) που είναι λίγο πιο πολύπλοκη και υποστηρίζει broadcasting. Αν προσπαθήσουμε να εκτελέσουμε αυτή τη πράξη με τα `features` και τα `weights` όπως είναι αυτή τη στιγμή, θα δούμε το παρακάτω σφάλμα\n",
    "\n",
    "```python\n",
    ">> torch.mm(features, weights)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "<ipython-input-13-15d592eb5279> in <module>()\n",
    "----> 1 torch.mm(features, weights)\n",
    "\n",
    "RuntimeError: size mismatch, m1: [1 x 5], m2: [1 x 5] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033\n",
    "```\n",
    "Καθώς θα δημιουργείτε νευρωνικά δίκτυα σε οποιοδήποτε library, το σφάλμα αυτό θα το βλέπετε συχνά. Πολύ συχνά. Αυτό που συμβαίνει γιατί οι τανυστές μας δεν έχουν τα σωστά shapes για να εκτελεστεί ο πολλαπλασιαμός πινάκων. Να θυμάστε ότι για τον πολλαπλασιασμό πινάκων, ο αριθμός των στηλών στον πρώτο τανυστή πρέπει να είναι ίσος με τον αριθμό των γραμμών του δεύτερου τανυστή. Και τα `features` και τα `weights` έχουν το ίδιο σχήμα, `(1, 5)`. Αυτό σημαίνει ότι πρέπει να αλλάξουμε τη μορφή του πίνακα `weights` για να εκτελέσουμε σωστά την πράξη του πολλαπλασιασμού πινάκων.\n",
    "\n",
    "**Note:** Για να δείτε τη μορφή (shape) ενός τανυστή με όνομα `tensor`, εκτελέστε `tensor.shape`. Εάν δημιουργείτε νευρωνικά δίκτυα, θα χρησιμοποιείτε συχνά αυτή τη μέθοδο.\n",
    "\n",
    "Υπάρχουν μερικές επιλογές εδώ: [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), and [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
    "\n",
    "* `weights.reshape(a, b)` θα επιστρέψει έναν νέο τανυστή με τα ίδια δεδομένα με το `weights` με μέγεθος `(a, b)` καποιες φορές, και μερικές φορές ως κλώνο, καθώς σε αυτό θα αντιγράφει τα δεδομένα σε ένα άλλο μέρος της μνήμης.\n",
    "* `weights.resize_(a, b)` επιστρέφει τον ίδιο τανυστή με διαφορετικό σχήμα. Ωστόσο, αν το νέο σχήμα έχει λιγότερα στοιχεία από τον αρχικό τανυστή, κάποια στοιχεία θα αφαιρεθούν από τον τανυστή (αλλά όχι από τη μνήμη). Αν το νέο σχήμα έχει περισσότερα στοιχεία από τον αρχικό τανυστή, τα νέα στοιχεία δεν θα αρχικοποιηθούν στη μνήμη. Εδώ πρέπει να σημειώσω ότι η υπογράμμιση στο τέλος της μεθόδου υποδηλώνει ότι αυτή η μέθοδος εκτελείται **in-place**. Εδώ είναι ένα μεγάλο thread σε ένα φόρουμ για να δείτε λεπτομέριες [read more about in-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) in PyTorch.\n",
    "* `weights.view(a, b)` θα επιστρέψει έναν νέο τανυστή με τα ίδια δεδομένα με το `weights` με μέγεθος `(a, b)`.\n",
    "\n",
    "Συνήθως χρησιμοποιώ τη μέθοδο `.view()`, αλλά οποιαδήποτε από τις τρεις μεθόδους θα λειτουργήσει σε αυτή τη περίπτωση. Έτσι, τώρα μπορώ να ΄΄αξω τη μορφή του πίνακα `weights` έτσι ώστε να έχει πέντε γραμμές και μία στήλα με μία εντολή όπως αυτή `weights.view(5, 1)`.\n",
    "\n",
    "> **Άσκηση**: Υπολογίστε την έξοδο του μικρού μας δικτύου χρησιμοποιώντας πολλαπλασιασμό πινάκων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Λ΄ύση\n",
    "\n",
    "y = activation(torch.mm(features, weights.view(5,1)) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ας τα συνδιάσουμε όλα!\n",
    "\n",
    "Με την παραπάνω εντολή μπορείτε να υπολογίσετε την έξοδο για έναν μόνο νευρώνα. Η πραγματική ισχύς αυτού του αλγορίθμου εμφανίζεται όταν αρχίζουμε να στοιβάζουμε όλους αυτούς τους επιμέρους κόμβους σε επίπεδα και στοίβες επιπέδων σε ένα δίκτυο νευρώνων. Η έξοδος ενός επιπέδου γίνεται η είσοδος για το επόμενο επίπεδο. Με αυτές τις πολλαπλές μονάδες εισόδου και μονάδες εξόδου, πρέπει τώρα να εκφράσουμε τα βάρη ως πίνακα.\n",
    "\n",
    "<img src='assets/multilayer_diagram_weights.png' width=450px>\n",
    "\n",
    "Το πρώτο επίπεδο που εμφανίζεται στο κάτω μέρος είναι οι είσοδοι, που κατά κανόνα ονομάζεται **επίπεδο εισόδου**. Το μεσαίο επίπεδο ονομάζεται **κρυφό επίπεδο** και το τελικό επίπεδο (πάνω) είναι το **επίπεδο εξόδου**. Μπορούμε εκ νέου να εκφράσουμε αυτό το δίκτυο μαθηματικά με πίνακες και να χρησιμοποιήσουμε πολλαπλασιασμούς πινάκων για να πάρουμε τους γραμμικούς συνδυασμούς τους για κάθε κόμβο σε μία εντολή. Για παράδειγμα, μπορεί να υπολογιστεί το κρυφό επίπεδο ($h_1$ και $h_2$) ως:\n",
    "\n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Η έξοδος για αυτό το μικρό δίκτυο υπολογίζεται αν λαβω το κρυφό επίπεδο σαν είσοδο του επιπέδου εξόδου. Η έξοδος του δικτύου εκφράζεται απλά:\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Δημιούργησε δεδομ΄ένα\n",
    "torch.manual_seed(7) # Ορίζω random seed για να μπορω να κανω ευκολα debug\n",
    "\n",
    "# Η είσοδος είναι 3 τυχαίες μεταβλητές απο μία κανονική κατανομή\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Καθορίστε το μέγεθος κάθε επιπέδου στο δίκτυό μας\n",
    "n_input = features.shape[1]     # Ο αριθμός των κόμβων εισόδου πρέπει να ταιριάζει με τον αριθμό των χαρακτηριστικών εισόδου\n",
    "n_hidden = 2                    # Αριθμός κόμβων κρυφού επιπέδου \n",
    "n_output = 1                    # Αριθμός κ΄όμβων εξόδου\n",
    "\n",
    "# Τα βάρη μεταξύ του επιπέδου εισόδου και του κρυφού επιπέδου\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Τα βάρη μεταξύ του κρυφού επι΄΄πέδου και του επι΄΄πεδου εξόδου\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# και οι τιμές των bias για τα κρυφό επίδεδο και το επίπεδο εξόδου \n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Άσκηση:** Υπολογίστε την έξοδο για αυτό το δίκτυο πολλών επιπέδων χρησιμοποιώντας τα βάρη `W1` & `W2`, και τις πολώσεις, `B1` & `B2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Λ΄υση\n",
    "\n",
    "h = activation(torch.mm(features, W1) + B1)\n",
    "print(h)\n",
    "output = activation(torch.mm(h, W2) + B2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εάν το κάνατε σωστά, θα πρέπει να δείτε την έξοδο `tensor([[ 0.3171]])`.\n",
    "\n",
    "Ο αριθμός των κρυφών επιπέδων είναι μια παράμετρος του δικτύου, που ονομάζεται συχνά **υπερπαράμετρος** για να  διαφοροποιήθει από τις παραμέτρους των βαρών και πόλωσης. Όπως θα δούμε αργότερα, όταν συζητάμε για την εκπαίδευση ενός νευρικού δικτύου, όσο πιο πολλούς κόμβους έχει ένα δίκτυο και όσο περισσότερα επίπεδα, τόσο πιο ικανά είναι να μάθει από τα δεδομένα και να κάνει ακριβείς προβλέψεις."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy σε Torch και πίσω\n",
    "\n",
    "Η PyTorch έχει ένα ωραίο χαρακτηριστικό για τη μετατροπή μεταξύ πινάκων Numpy και Torch τανυστών. Για να δημιουργήσετε έναν τανυστή από έναν πίνακα Numpy, εκτελέστε `torch.from_numpy()`. Για να μετατρέψετε έναν τανυστή σε έναν πίνακα Numpy, χρησιμοποιείστε τη μέθοδο `.numpy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η μνήμη είναι κοινή μεταξύ του αριθμητικού πίνακα και του τανυστή Torch, οπότε αν αλλάξετε τις τιμές του ενός αντικειμένου, θα αλλαξουν και οι τιμές του άλλου."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply PyTorch Tensor by 2, in place\n",
    "b.mul_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy array matches new values from Tensor\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτό το notebook 📖 δημιουργήθηκε για το μάθημα ***Υπολογιστική Νοημοσύνη και Μηχανική Μάθηση*** του Τμήματος Μηχανικών Παραγωγής και Διοίκησης, της Πολυτεχνικής Σχολής του Δημοκριτείου Πανεπιστημίου Θράκης.<br>\n",
    "This notebook is made available under the Creative Commons Attribution [(CC-BY)](https://creativecommons.org/licenses/by/4.0/legalcode) license. Code is also made available under the [MIT License](https://opensource.org/licenses/MIT).<br>\n",
    "Author: Asst. Prof. Angelos Amanatiadis\n",
    "<img src=\"assets/cc.png\" style=\"width:55px; float: right; margin: 0px 0px 0px 0px;\"></img>\n",
    "<img src=\"assets/mit.png\" style=\"width:40px; float: right; margin: 0px 10px 0px 0px;\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
